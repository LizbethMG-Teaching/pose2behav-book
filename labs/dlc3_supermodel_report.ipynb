{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72f9613",
   "metadata": {},
   "source": [
    "# DeepLabCut 3 (SuperModel) — Output Analyzer\n",
    "\n",
    "This notebook discovers and analyzes the output files produced by DeepLabCut 3 (SuperModel) when you analyze a video.\n",
    "\n",
    "**What you'll get:**\n",
    "- Automatic file discovery (HDF5 predictions, labeled videos, before/after adapt JSONs, model snapshots)\n",
    "- Robust HDF5 loader that handles multiple DLC schema variants\n",
    "- Video-specific tracking quality metrics derived from likelihoods\n",
    "- Diagnostics plots (likelihoods, per-bodypart coverage, inter-frame motion, smoothness)\n",
    "- JSON diff of `*_before_adapt.json` vs `*_after_adapt.json`\n",
    "- Final summary report cell you can export/print\n",
    "\n",
    "> Tip: Run cells from top to bottom. Set `BASE_DIR` below to where your DLC outputs live.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ee221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Configure this path to the folder that contains your DLC 3 SuperModel outputs\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path('.')  # <- change me to your output folder if needed, e.g., Path('/Users/you/experiments/run_01')\n",
    "\n",
    "# Optional: If your outputs are scattered, you can add extra directories here:\n",
    "EXTRA_DIRS = []  # e.g., [Path('/another/folder/of/outputs')]\n",
    "\n",
    "# Likelihood threshold(s) used for quality metrics\n",
    "LIKELIHOOD_THRESHOLDS = [0.9, 0.8, 0.5]\n",
    "\n",
    "# If you have an expected frame rate (fps) for speed estimates, set it here; otherwise we infer if possible\n",
    "FPS_HINT = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10022fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys, json, math, textwrap, itertools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# Plotting (matplotlib only, one chart per figure, default colors)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "except Exception as e:\n",
    "    cv2 = None\n",
    "\n",
    "def find_output_files(base: Path, extras=()):\n",
    "    bases = [base] + list(extras)\n",
    "    files = []\n",
    "    for b in bases:\n",
    "        if b.exists():\n",
    "            for p in b.rglob('*'):\n",
    "                if p.is_file():\n",
    "                    files.append(p)\n",
    "    # Classify\n",
    "    h5_preds = [p for p in files if p.suffix.lower()=='.h5' and 'snapshot' not in p.name.lower()]\n",
    "    h5_snapshots = [p for p in files if p.suffix.lower()=='.h5' and 'snapshot' in p.name.lower()]\n",
    "    jsons = [p for p in files if p.suffix.lower()=='.json']\n",
    "    labeled_vids = [p for p in files if p.suffix.lower() in ('.mp4','.avi','.mov') and 'labeled' in p.stem.lower()]\n",
    "    other_vids = [p for p in files if p.suffix.lower() in ('.mp4','.avi','.mov') and 'labeled' not in p.stem.lower()]\n",
    "    eval_candidates = [p for p in files if p.name.lower().startswith('evaluation-results') and p.suffix.lower() in ('.pkl','.csv','.json')]\n",
    "    return {\n",
    "        'all': files,\n",
    "        'h5_predictions': h5_preds,\n",
    "        'h5_snapshots': h5_snapshots,\n",
    "        'jsons': jsons,\n",
    "        'labeled_videos': labeled_vids,\n",
    "        'source_videos': other_vids,\n",
    "        'eval_results': eval_candidates\n",
    "    }\n",
    "\n",
    "def read_any_h5_predictions(path: Path):\n",
    "    \"\"\"Robustly read DLC predictions from HDF5, returning (df, meta) where df has columns:\n",
    "        MultiIndex or flat: (bodypart, coord) or bodypart_coord\n",
    "        coords are typically ['x','y','likelihood'] (p)\n",
    "    \"\"\"\n",
    "    meta = {}\n",
    "    # Try pandas first (common for DLC)\n",
    "    try:\n",
    "        with pd.HDFStore(path, 'r') as store:\n",
    "            keys = store.keys()\n",
    "            meta['keys'] = list(keys)\n",
    "            # Heuristic: pick the largest key table\n",
    "            sizes = {}\n",
    "            for k in keys:\n",
    "                try:\n",
    "                    n = store.get_storer(k).nrows\n",
    "                except Exception:\n",
    "                    n = None\n",
    "                sizes[k] = n\n",
    "            # prefer '/df_with_missing' then largest\n",
    "            key = '/df_with_missing' if '/df_with_missing' in keys else max(keys, key=lambda k: sizes.get(k, -1))\n",
    "            df = store.select(key)\n",
    "            meta['selected_key'] = key\n",
    "    except Exception as e:\n",
    "        # Fallback: use h5py + manual conversion\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            meta['h5py_keys'] = list(f.keys())\n",
    "            # Try common dataset name guesses\n",
    "            candidate = None\n",
    "            for guess in ['df_with_missing','table','data','/','/df_with_missing']:\n",
    "                if guess in f:\n",
    "                    candidate = guess\n",
    "                    break\n",
    "            if candidate is None:\n",
    "                # pick first dataset-ish\n",
    "                for k in f.keys():\n",
    "                    if isinstance(f[k], h5py.Dataset):\n",
    "                        candidate = k\n",
    "                        break\n",
    "            if candidate is None:\n",
    "                raise RuntimeError(f\"Could not find a dataset inside {path}\")\n",
    "            data = f[candidate][()]\n",
    "            df = pd.DataFrame(data)\n",
    "            meta['selected_key'] = candidate\n",
    "    # Normalize columns\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        cols = df.columns\n",
    "    else:\n",
    "        # Attempt to parse 'bodypart_x' style\n",
    "        new_cols = []\n",
    "        for c in df.columns:\n",
    "            s = str(c)\n",
    "            if any(s.endswith('_'+coord) for coord in ['x','y','likelihood','p','prob','conf','confidence']):\n",
    "                for coord in ['x','y','likelihood','p','prob','conf','confidence']:\n",
    "                    if s.endswith('_'+coord):\n",
    "                        bp = s[:-(len(coord)+1)]\n",
    "                        new_cols.append((bp, 'likelihood' if coord in ['p','prob','conf','confidence'] else coord))\n",
    "                        break\n",
    "            else:\n",
    "                # keep as is under 'meta'\n",
    "                new_cols.append(('meta', s))\n",
    "        df.columns = pd.MultiIndex.from_tuples(new_cols)\n",
    "    # Unify likelihood label\n",
    "    def _norm_coord(c):\n",
    "        return 'likelihood' if c in ['p','prob','conf','confidence'] else c\n",
    "    df.columns = pd.MultiIndex.from_tuples([(bp, _norm_coord(coord)) for bp,coord in df.columns])\n",
    "    return df, meta\n",
    "\n",
    "def estimate_fps_from_video(video_path: Path):\n",
    "    if cv2 is None:\n",
    "        return None\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "    if fps and fps > 0:\n",
    "        return fps\n",
    "    return None\n",
    "\n",
    "def pairwise_diff(a):\n",
    "    return np.abs(np.diff(a.astype(float), axis=0))\n",
    "\n",
    "def smoothness_metric(series, window=5):\n",
    "    # Simple smoothness: mean absolute second derivative magnitude\n",
    "    s = np.array(series, dtype=float)\n",
    "    if len(s) < window+2:\n",
    "        return np.nan\n",
    "    v = np.diff(s)\n",
    "    a = np.diff(v)\n",
    "    return float(np.nanmean(np.abs(a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e8dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = find_output_files(BASE_DIR, EXTRA_DIRS)\n",
    "print('Found:')\n",
    "for k,v in files.items():\n",
    "    print(f'  {k}: {len(v)}')\n",
    "    for p in v[:5]:\n",
    "        print('   -', p)\n",
    "    if len(v)>5:\n",
    "        print('   ...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba175d0",
   "metadata": {},
   "source": [
    "## Inspect model metadata (before/after adapt JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b2b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, difflib\n",
    "\n",
    "before_json = [p for p in files['jsons'] if 'before_adapt' in p.name]\n",
    "after_json  = [p for p in files['jsons'] if 'after_adapt'  in p.name]\n",
    "\n",
    "def load_json_safe(p):\n",
    "    try:\n",
    "        return json.loads(Path(p).read_text())\n",
    "    except Exception as e:\n",
    "        return {'_error': str(e)}\n",
    "\n",
    "before = load_json_safe(before_json[0]) if before_json else None\n",
    "after  = load_json_safe(after_json[0]) if after_json else None\n",
    "\n",
    "print('Before adapt JSON:', before_json[0] if before else 'None')\n",
    "print('After adapt  JSON:', after_json[0] if after  else 'None')\n",
    "\n",
    "if before and after:\n",
    "    before_txt = json.dumps(before, indent=2, sort_keys=True)\n",
    "    after_txt  = json.dumps(after,  indent=2, sort_keys=True)\n",
    "    diff = list(difflib.unified_diff(before_txt.splitlines(), after_txt.splitlines(), lineterm=''))\n",
    "    print('\\nUnified diff (truncated to 300 lines):')\n",
    "    for line in diff[:300]:\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca64c15",
   "metadata": {},
   "source": [
    "## Load prediction HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_handles = []\n",
    "for h5 in files['h5_predictions']:\n",
    "    try:\n",
    "        df, meta = read_any_h5_predictions(h5)\n",
    "        # Heuristic: drop all-'meta' columns if any\n",
    "        if all(bp=='meta' for bp,_ in df.columns):\n",
    "            continue\n",
    "        pred_handles.append((h5, df, meta))\n",
    "        print(f'Loaded: {h5}  | shape={df.shape}  | key={meta.get(\"selected_key\")}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load {h5}: {e}')\n",
    "\n",
    "if not pred_handles:\n",
    "    print('No prediction HDF5 files could be loaded. Check BASE_DIR or file formats.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8f488",
   "metadata": {},
   "source": [
    "## Compute video-specific quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "per_bp_rows = []\n",
    "\n",
    "for h5, df, meta in pred_handles:\n",
    "    # Identify coords\n",
    "    coords = sorted(set(c for _,c in df.columns))\n",
    "    # Expect x,y,likelihood\n",
    "    if 'likelihood' not in coords:\n",
    "        print(f'Warning: no likelihood column detected in {h5.name}; metrics will be limited.')\n",
    "    n_frames = len(df)\n",
    "    bodyparts = sorted(set(bp for bp,_ in df.columns if bp!='meta'))\n",
    "\n",
    "    # Basic stats\n",
    "    for bp in bodyparts:\n",
    "        cols = df[bp]\n",
    "        has_like = 'likelihood' in cols\n",
    "        mean_like = float(cols['likelihood'].mean()) if has_like else np.nan\n",
    "        med_like  = float(cols['likelihood'].median()) if has_like else np.nan\n",
    "        coverage = {}\n",
    "        if has_like:\n",
    "            for thr in LIKELIHOOD_THRESHOLDS:\n",
    "                coverage[thr] = float((cols['likelihood']>=thr).mean())\n",
    "        # Inter-frame motion (pixels/frame)\n",
    "        if {'x','y'}.issubset(cols.columns):\n",
    "            xy = cols[['x','y']].values\n",
    "            diffs = pairwise_diff(xy)\n",
    "            speed_pf = np.linalg.norm(diffs, axis=1)  # pixels per frame\n",
    "            mean_speed = float(np.nanmean(speed_pf))\n",
    "            smoothness = smoothness_metric(cols['x']) + smoothness_metric(cols['y'])\n",
    "        else:\n",
    "            mean_speed = np.nan\n",
    "            smoothness = np.nan\n",
    "\n",
    "        per_bp_rows.append({\n",
    "            'file': h5.name,\n",
    "            'frames': n_frames,\n",
    "            'bodypart': bp,\n",
    "            'mean_likelihood': mean_like,\n",
    "            'median_likelihood': med_like,\n",
    "            **{f'coverage_ge_{thr}': coverage.get(thr, np.nan) for thr in LIKELIHOOD_THRESHOLDS},\n",
    "            'mean_speed_px_per_frame': mean_speed,\n",
    "            'smoothness_metric': smoothness\n",
    "        })\n",
    "\n",
    "    # File-level summary\n",
    "    mean_like_all = float(np.nanmean([r['mean_likelihood'] for r in per_bp_rows if r['file']==h5.name]))\n",
    "    summary_rows.append({\n",
    "        'file': h5.name,\n",
    "        'frames': n_frames,\n",
    "        'n_bodyparts': len(bodyparts),\n",
    "        'mean_likelihood_all_bps': mean_like_all\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "per_bp_df = pd.DataFrame(per_bp_rows)\n",
    "\n",
    "from caas_jupyter_tools import display_dataframe_to_user\n",
    "if not summary_df.empty:\n",
    "    display_dataframe_to_user('DLC3 summary by file', summary_df)\n",
    "if not per_bp_df.empty:\n",
    "    display_dataframe_to_user('DLC3 per-bodypart metrics', per_bp_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ba2e0",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b62bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood distribution per file (all bodyparts pooled)\n",
    "for h5, df, meta in pred_handles:\n",
    "    like_cols = [df[bp]['likelihood'] for bp,_c in df.columns.unique(level=0) if bp!='meta' and 'likelihood' in df[bp]]\n",
    "    if not like_cols:\n",
    "        continue\n",
    "    like = pd.concat(like_cols, axis=0).dropna()\n",
    "    plt.figure()\n",
    "    like.hist(bins=50)\n",
    "    plt.title(f'Likelihood distribution — {h5.name}')\n",
    "    plt.xlabel('likelihood')\n",
    "    plt.ylabel('count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e107f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage per bodypart at threshold 0.9 (first file only for simplicity)\n",
    "if pred_handles:\n",
    "    h5, df, meta = pred_handles[0]\n",
    "    bp_cov = []\n",
    "    for bp in sorted(set(bp for bp,_ in df.columns if bp!='meta')):\n",
    "        if 'likelihood' in df[bp]:\n",
    "            cov = float((df[bp]['likelihood']>=0.9).mean())\n",
    "            bp_cov.append((bp, cov))\n",
    "    if bp_cov:\n",
    "        labels, vals = zip(*bp_cov)\n",
    "        plt.figure()\n",
    "        plt.bar(range(len(vals)), vals)\n",
    "        plt.xticks(range(len(vals)), labels, rotation=45, ha='right')\n",
    "        plt.title(f'Coverage (likelihood ≥ 0.9) per bodypart — {h5.name}')\n",
    "        plt.ylabel('fraction of frames')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c223dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter-frame displacement (speed) over time for a chosen bodypart (first file, first bp)\n",
    "if pred_handles:\n",
    "    h5, df, meta = pred_handles[0]\n",
    "    bodyparts = sorted(set(bp for bp,_ in df.columns if bp!='meta'))\n",
    "    if bodyparts and {'x','y'}.issubset(df[bodyparts[0]].columns):\n",
    "        xy = df[bodyparts[0]][['x','y']].values\n",
    "        spf = np.linalg.norm(np.diff(xy, axis=0), axis=1)\n",
    "        plt.figure()\n",
    "        plt.plot(spf)\n",
    "        plt.title(f'Inter-frame displacement (pixels/frame) — {h5.name} — {bodyparts[0]}')\n",
    "        plt.xlabel('frame')\n",
    "        plt.ylabel('pixels/frame')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5fd01",
   "metadata": {},
   "source": [
    "## (Optional) Infer FPS from labeled video and add px/s speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb61b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = None\n",
    "if files['labeled_videos']:\n",
    "    fps = estimate_fps_from_video(files['labeled_videos'][0])\n",
    "if fps is None:\n",
    "    fps = FPS_HINT\n",
    "print('FPS used:', fps)\n",
    "\n",
    "if fps and pred_handles:\n",
    "    h5, df, meta = pred_handles[0]\n",
    "    bodyparts = sorted(set(bp for bp,_ in df.columns if bp!='meta'))\n",
    "    if bodyparts and {'x','y'}.issubset(df[bodyparts[0]].columns):\n",
    "        xy = df[bodyparts[0]][['x','y']].values\n",
    "        spf = np.linalg.norm(np.diff(xy, axis=0), axis=1)  # pixels/frame\n",
    "        sps = spf * fps\n",
    "        plt.figure()\n",
    "        plt.plot(sps)\n",
    "        plt.title(f'Estimated speed (pixels/second) — {h5.name} — {bodyparts[0]}')\n",
    "        plt.xlabel('frame')\n",
    "        plt.ylabel('pixels/second')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d63e7f",
   "metadata": {},
   "source": [
    "## (Optional) Model evaluation results (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rows = []\n",
    "for p in files['eval_results']:\n",
    "    try:\n",
    "        if p.suffix.lower()=='.csv':\n",
    "            edf = pd.read_csv(p)\n",
    "            edf['_source'] = p.name\n",
    "            eval_rows.append(edf)\n",
    "        elif p.suffix.lower()=='.json':\n",
    "            with open(p) as f:\n",
    "                data = json.load(f)\n",
    "            edf = pd.json_normalize(data)\n",
    "            edf['_source'] = p.name\n",
    "            eval_rows.append(edf)\n",
    "        else:\n",
    "            # pkl – try pandas\n",
    "            edf = pd.read_pickle(p)\n",
    "            if isinstance(edf, pd.DataFrame):\n",
    "                edf['_source'] = p.name\n",
    "                eval_rows.append(edf)\n",
    "            else:\n",
    "                edf = pd.DataFrame([{'_raw': str(edf), '_source': p.name}])\n",
    "                eval_rows.append(edf)\n",
    "        print('Loaded evaluation results:', p.name)\n",
    "    except Exception as e:\n",
    "        print('Failed to read eval results', p, e)\n",
    "\n",
    "if eval_rows:\n",
    "    eval_df = pd.concat(eval_rows, ignore_index=True, sort=False)\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user('DLC3 evaluation results (if any)', eval_df)\n",
    "else:\n",
    "    print('No evaluation results files found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750bc202",
   "metadata": {},
   "source": [
    "## Final summary (printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_lines = []\n",
    "report_lines.append('# DLC3 (SuperModel) — Analysis Report')\n",
    "report_lines.append('')\n",
    "report_lines.append('## Files discovered')\n",
    "for k in ['h5_predictions','jsons','labeled_videos','h5_snapshots','eval_results']:\n",
    "    report_lines.append(f'### {k}')\n",
    "    for p in files[k]:\n",
    "        report_lines.append(f'- {p.name}')\n",
    "    if not files[k]:\n",
    "        report_lines.append('- (none)')\n",
    "report_lines.append('')\n",
    "\n",
    "if not summary_df.empty:\n",
    "    report_lines.append('## File-level quality (likelihood-based)')\n",
    "    for _,row in summary_df.iterrows():\n",
    "        report_lines.append(f\"- **{row['file']}**: frames={int(row['frames'])}, bodyparts={int(row['n_bodyparts'])}, mean likelihood (all bps)={row['mean_likelihood_all_bps']:.3f}\")\n",
    "    report_lines.append('')\n",
    "\n",
    "if not per_bp_df.empty:\n",
    "    report_lines.append('## Per-bodypart metrics (first few rows)')\n",
    "    head = per_bp_df.head(12)\n",
    "    report_lines.append(head.to_markdown(index=False))\n",
    "\n",
    "if before and after:\n",
    "    report_lines.append('')\n",
    "    report_lines.append('## Notable changes after adaptation (JSON diff present above)')\n",
    "else:\n",
    "    report_lines.append('')\n",
    "    report_lines.append('## Adaptation JSONs not both present; skip diff.')\n",
    "\n",
    "print('\\n'.join(report_lines))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
